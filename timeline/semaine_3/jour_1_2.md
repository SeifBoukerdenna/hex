### Semaine 3: Minimax et Alpha-Beta Pruning

#### ‚úÖ T√¢ches compl√©t√©es

- [x] Revoir notes de cours sur minimax
- [x] Comprendre alternance max/min
- [x] Impl√©menter minimax v0.4 (profondeur 2)
- [x] Constater que v0.4 est trop lent (~5+ min par partie)
- [x] Impl√©menter alpha-beta pruning v0.5
- [x] Tester et valider les performances

---

## Version 0.4: Minimax (trop lent)

### Impl√©mentation

```python
def minimax(self, state, depth, maximizing):
    if depth == 0 or state.is_done():
        return self.evaluate(state)

    possible_actions = list(state.get_possible_light_actions())

    if maximizing:
        max_eval = float('-inf')
        for action in possible_actions:
            next_state = state.apply_action(action)
            eval_score = self.minimax(next_state, depth - 1, False)
            max_eval = max(max_eval, eval_score)
        return max_eval
    else:
        min_eval = float('inf')
        for action in possible_actions:
            next_state = state.apply_action(action)
            eval_score = self.minimax(next_state, depth - 1, True)
            min_eval = min(min_eval, eval_score)
        return min_eval
```

### Probl√®me de performance

**Analyse de complexit√©:**

- Plateau 14√ó14 = 196 cases
- D√©but de partie: ~196 actions possibles
- Profondeur 2: 196 √ó 195 = **38,220 positions √† √©valuer**
- Chaque √©valuation = 2 Dijkstra (moi + adversaire)
- R√©sultat: **>5 minutes par coup** en d√©but de partie

**Constat:** Minimax pur avec profondeur 2 est inutilisable sur un plateau 14√ó14. La complexit√© O(b^d) o√π b=196 et d=2 est trop √©lev√©e sans optimisation.

---

## Version 0.5: Alpha-Beta Pruning

### Principe de l'√©lagage alpha-beta

L'alpha-beta pruning √©vite d'explorer des branches dont on sait d√©j√† qu'elles ne peuvent pas am√©liorer le r√©sultat:

- **Alpha**: meilleur score garanti pour le joueur MAX (moi)
- **Beta**: meilleur score garanti pour le joueur MIN (adversaire)
- **Coupure**: si beta ‚â§ alpha, on arr√™te d'explorer cette branche

**Gain th√©orique:** Dans le meilleur cas, r√©duit la complexit√© de O(b^d) √† O(b^(d/2)), soit de 38,220 √† ~196 positions.

### Impl√©mentation

```python
def alphabeta(self, state, depth, alpha, beta, maximizing):
    if depth == 0 or state.is_done():
        return self.evaluate(state)

    possible_actions = list(state.get_possible_light_actions())

    # Tri des actions: centre d'abord (am√©liore le pruning)
    possible_actions.sort(key=lambda a:
        abs(a.data["position"][0] - 6.5) + abs(a.data["position"][1] - 6.5))

    if maximizing:
        max_eval = float('-inf')
        for action in possible_actions:
            next_state = state.apply_action(action)
            eval_score = self.alphabeta(next_state, depth - 1, alpha, beta, False)
            max_eval = max(max_eval, eval_score)
            alpha = max(alpha, eval_score)
            if beta <= alpha:
                break  # Coupure beta
        return max_eval
    else:
        min_eval = float('inf')
        for action in possible_actions:
            next_state = state.apply_action(action)
            eval_score = self.alphabeta(next_state, depth - 1, alpha, beta, True)
            min_eval = min(min_eval, eval_score)
            beta = min(beta, eval_score)
            if beta <= alpha:
                break  # Coupure alpha
        return min_eval
```

### Optimisation: Tri des actions

**Pourquoi trier par distance au centre?**

- Les coups centraux sont g√©n√©ralement meilleurs dans Hex
- Explorer les meilleurs coups en premier maximise les coupures
- Plus de coupures = moins de positions √† √©valuer = plus rapide

```python
possible_actions.sort(key=lambda a:
    abs(a.data["position"][0] - 6.5) + abs(a.data["position"][1] - 6.5))
```

---

## üìä R√©sultats des tests v0.5

### TEST 1: v0.5 vs Random Player (10 parties)

**Configuration 1 - Mon agent joue Rouge (premier):**
| Partie | R√©sultat | Temps |
|--------|----------|-------|
| 1 | ‚úì Victoire | 21.4s |
| 2 | ‚úì Victoire | 23.7s |
| 3 | ‚úì Victoire | 19.2s |
| 4 | ‚úì Victoire | 40.5s |
| 5 | ‚úì Victoire | 26.5s |
| **Total** | **5/5 (100%)** | **Moy: 26.3s** |

**Configuration 2 - Mon agent joue Bleu (second):**
| Partie | R√©sultat | Temps |
|--------|----------|-------|
| 1 | ‚úì Victoire | 24.3s |
| 2 | ‚úì Victoire | 21.8s |
| 3 | ‚úì Victoire | 28.0s |
| 4 | ‚úì Victoire | 50.2s |
| 5 | ‚úì Victoire | 39.1s |
| **Total** | **5/5 (100%)** | **Moy: 32.7s** |

**R√©sultat global vs Random: 10/10 (100%)**

### TEST 2: v0.5 vs Greedy Player (10 parties)

**Configuration 1 - Mon agent joue Rouge (premier):**
| Partie | R√©sultat | Temps |
|--------|----------|-------|
| 1 | ‚úì Victoire | 26.3s |
| 2 | ‚úì Victoire | 28.8s |
| 3 | ‚úì Victoire | 30.3s |
| 4 | ‚úì Victoire | 24.6s |
| 5 | ‚úì Victoire | 29.8s |
| **Total** | **5/5 (100%)** | **Moy: 28.0s** |

**Configuration 2 - Mon agent joue Bleu (second):**
| Partie | R√©sultat | Temps |
|--------|----------|-------|
| 1 | ‚úì Victoire | 36.1s |
| 2 | ‚úì Victoire | 36.1s |
| 3 | ‚úì Victoire | 37.0s |
| 4 | ‚úì Victoire | 35.7s |
| 5 | ‚úì Victoire | 37.2s |
| **Total** | **5/5 (100%)** | **Moy: 36.4s** |

**R√©sultat global vs Greedy: 10/10 (100%)**

---

## üìà Tableau comparatif des versions

| Version                | vs Random | vs Greedy | Temps/partie | Strat√©gie                  |
| ---------------------- | --------- | --------- | ------------ | -------------------------- |
| v0.1 (random)          | 20%       | 0%        | ~2s          | Aucune                     |
| v0.2 (greedy clone)    | 50%       | 50%       | ~0.5s        | Offense pure               |
| v0.3 (offense+d√©fense) | 50%       | 50%       | ~3s          | Heuristique sans lookahead |
| v0.4 (minimax)         | N/A       | N/A       | >300s        | Trop lent                  |
| **v0.5 (alpha-beta)**  | **100%**  | **100%**  | **~30s**     | Minimax optimis√©           |

### Progression observ√©e

```
v0.1 ‚Üí v0.2: +30% vs random (strat√©gie > hasard)
v0.2 ‚Üí v0.3: +0% (lookahead n√©cessaire pour battre greedy)
v0.3 ‚Üí v0.5: +50% vs greedy (lookahead permet d'anticiper)
```

---

## ü§î Analyse et r√©flexions

### Pourquoi v0.5 bat greedy √† 100%?

**1. Anticipation vs r√©action**

- Greedy ne regarde que 1 coup en avance (son propre chemin)
- v0.5 regarde 2 coups: mon coup + r√©ponse adverse
- Cette anticipation permet de bloquer les menaces AVANT qu'elles ne soient critiques

**2. Exploitation de la faiblesse du greedy**

- Greedy ne d√©fend JAMAIS - il construit uniquement son chemin
- v0.5 d√©tecte quand greedy est proche de gagner et bloque
- Greedy continue aveugl√©ment m√™me quand son chemin est coup√©

**3. L'heuristique offensive+d√©fensive**

```python
score = distance_adversaire - ma_distance
```

- Si je bloque l'adversaire, sa distance AUGMENTE ‚Üí mon score augmente
- Minimax choisit les coups qui maximisent ce score apr√®s r√©ponse adverse
- R√©sultat: coups qui avancent ET bloquent simultan√©ment

### Pourquoi le temps varie (19s √† 50s)?

**Facteurs influen√ßant le temps de calcul:**

1. **Position dans la partie**

   - D√©but: ~196 actions ‚Üí plus lent
   - Fin: ~50 actions ‚Üí plus rapide

2. **Efficacit√© du pruning**

   - Bon ordre de tri ‚Üí beaucoup de coupures ‚Üí rapide
   - Mauvais ordre ‚Üí peu de coupures ‚Üí lent

3. **Complexit√© de la position**
   - Positions "√©videntes" ‚Üí coupures rapides
   - Positions √©quilibr√©es ‚Üí plus d'exploration n√©cessaire

**Observation:** Les parties contre greedy en second (36s moy) sont plus longues que contre random (26-32s). Hypoth√®se: greedy cr√©e des positions plus complexes √† √©valuer.

### Analyse du temps par configuration

| Adversaire | Config Rouge (1er) | Config Bleu (2nd) | Diff√©rence |
| ---------- | ------------------ | ----------------- | ---------- |
| Random     | 26.3s              | 32.7s             | +6.4s      |
| Greedy     | 28.0s              | 36.4s             | +8.4s      |

**Interpr√©tation:** Jouer en second prend plus de temps car:

- L'adversaire a d√©j√† une pi√®ce ‚Üí position plus complexe
- Besoin de calculer comment rattraper l'avantage
- Plus de "menaces" √† √©valuer dans l'heuristique d√©fensive

---

## üí° Insights techniques

### L'importance du tri des actions

**Sans tri:** Explore dans un ordre arbitraire, peu de coupures
**Avec tri (centre d'abord):** Les meilleurs coups sont explor√©s en premier

**Impact mesur√©:**

- Sans tri: temps moyen estim√© ~60-90s/partie
- Avec tri: temps moyen ~30s/partie
- **Gain: ~50-60% de r√©duction du temps**

### Profondeur 2 est-elle suffisante?

**Arguments pour profondeur 2:**

- Bat greedy √† 100% ‚Üí objectif atteint
- Temps acceptable (~30s) vs budget de 15 min total
- Plus de profondeur = exponentiellement plus lent

**Arguments pour augmenter:**

- Adversaires plus forts pourraient n√©cessiter plus de lookahead
- Profondeur 3 pourrait d√©tecter des menaces √† plus long terme

**D√©cision:** Rester √† profondeur 2 pour l'instant, envisager iterative deepening pour la suite.

### Gestion du temps de jeu

**Budget total:** 15 minutes = 900 secondes
**Temps moyen par coup:** ~30s / partie ‚âà quelques secondes par coup
**Nombre de coups typique:** 40-80 coups par partie

**Estimation:**

- 60 coups √ó 1-2s/coup = 60-120s total
- Marge confortable vs les 900s disponibles

**Optimisation future possible:** Iterative deepening

- Commencer √† profondeur 1
- Augmenter tant qu'il reste du temps
- Retourner le meilleur coup trouv√© quand le temps est √©coul√©

---

## üêõ Probl√®mes rencontr√©s et solutions

### Probl√®me 1: Minimax trop lent

**Sympt√¥me:** Une seule partie prenait >5 minutes
**Cause:** 196¬≤ = 38,000 positions √† √©valuer sans pruning
**Solution:** Impl√©menter alpha-beta pruning + tri des actions
**Le√ßon:** Toujours consid√©rer la complexit√© algorithmique avant d'impl√©menter

### Probl√®me 2: D√©terminer qui maximise/minimise

**Sympt√¥me:** Confusion sur quand utiliser max vs min
**Cause:** Le joueur qui appelle minimax veut maximiser SON score
**Solution:**

- Premier appel apr√®s mon coup ‚Üí adversaire joue ‚Üí `maximizing=False`
- Puis alterner √† chaque profondeur
  **Le√ßon:** Dessiner l'arbre de jeu sur papier aide √©norm√©ment

### Probl√®me 3: Ordre des actions affecte le pruning

**Sympt√¥me:** Temps de calcul tr√®s variable (19s √† 50s)
**Cause:** Mauvais ordre = peu de coupures alpha-beta
**Solution:** Trier les actions par distance au centre
**Le√ßon:** L'ordre d'exploration est crucial pour alpha-beta

---

## üìù Code final v0.5

**Structure:**

```
my_player.py (v0.5)
‚îú‚îÄ‚îÄ __init__: initialise piece_type, opponent_type, max_depth=2
‚îú‚îÄ‚îÄ compute_action: point d'entr√©e, lance alpha-beta sur chaque action
‚îú‚îÄ‚îÄ alphabeta: minimax avec √©lagage alpha-beta
‚îú‚îÄ‚îÄ evaluate: heuristique (distance_adversaire - ma_distance)
‚îî‚îÄ‚îÄ calculate_shortest_path: Dijkstra pour calculer distances
```

**Param√®tres configurables:**

- `max_depth = 2`: profondeur de recherche
- Tri par centre: optimise le pruning

**D√©pendances:**

```python
import numpy as np   # Pour matrices de distance
import heapq        # Pour Dijkstra (priority queue)
```

---

## üéØ Objectifs atteints vs planifi√©s

| Objectif Semaine 3    | Statut | R√©sultat                          |
| --------------------- | ------ | --------------------------------- |
| Impl√©menter minimax   | ‚úÖ     | v0.4 fonctionnel mais trop lent   |
| Tester profondeur 1-2 | ‚úÖ     | Profondeur 2 n√©cessite alpha-beta |
| Battre greedy         | ‚úÖ     | 100% victoires                    |
| Temps <60s/partie     | ‚úÖ     | ~30s/partie en moyenne            |

**Objectifs d√©pass√©s:**

- 100% vs greedy (objectif √©tait >50%)
- 100% vs random (objectif √©tait >90%)
- Alpha-beta impl√©ment√© (pr√©vu semaine 4)

---

## üöÄ Prochaines √©tapes (Semaine 4+)

### Optimisations envisageables

1. **Iterative deepening**

   - Augmenter profondeur tant qu'il reste du temps
   - Permet d'utiliser tout le budget de 15 min intelligemment

2. **Transposition table**

   - Cache les positions d√©j√† √©valu√©es
   - √âvite de recalculer les m√™mes sous-arbres

3. **Meilleur tri des actions**

   - Utiliser l'heuristique pour trier (pas juste distance au centre)
   - "Killer move heuristic": m√©moriser les bons coups

4. **Am√©liorer l'heuristique**
   - Bonus pour contr√¥le du centre
   - Bonus pour groupes connect√©s
   - P√©nalit√© pour pi√®ces isol√©es

### Tests √† faire

- [ ] Tester contre d'autres agents sur Abyss
- [ ] Mesurer pr√©cis√©ment le nombre de n≈ìuds explor√©s
- [ ] Comparer profondeur 2 vs 3 (avec iterative deepening)
- [ ] Identifier les types de positions o√π v0.5 pourrait perdre

---

## üí≠ R√©flexions finales

### Ce que j'ai appris

**Sur l'algorithme minimax:**

- Conceptuellement simple mais exponentiellement co√ªteux
- L'√©lagage alpha-beta est ESSENTIEL, pas optionnel
- L'ordre d'exploration impacte drastiquement la performance

**Sur le jeu Hex:**

- Profondeur 2 suffit pour battre un agent greedy
- L'anticipation (lookahead) est la cl√© pour gagner
- Un bon agent doit √©quilibrer offense et d√©fense

**Sur le d√©veloppement:**

- Tester t√¥t r√©v√®le les probl√®mes de performance
- La progression it√©rative (v0.1 ‚Üí v0.5) permet de diagnostiquer
- Chaque optimisation doit √™tre mesur√©e, pas assum√©e

### Comparaison avec les objectifs du projet

Selon le PDF du projet, l'approche "minimax avec bonne heuristique" est recommand√©e au niveau standard et a gagn√© le concours en 2021, 2022 et 2023. Ma v0.5 suit exactement cette approche:

- ‚úÖ Minimax avec alpha-beta
- ‚úÖ Heuristique bas√©e sur shortest path (comme greedy mais bidirectionnelle)
- ‚úÖ Gestion du temps acceptable

**Prochaine priorit√©:** Tester sur Abyss pour voir comment v0.5 se comporte contre de vrais adversaires.

---

## ‚è±Ô∏è Temps pass√©

| T√¢che                              | Temps     |
| ---------------------------------- | --------- |
| Th√©orie minimax (r√©vision cours)   | ~1h       |
| Impl√©mentation v0.4 (minimax)      | ~1h       |
| Diagnostic probl√®me de performance | ~30min    |
| Impl√©mentation v0.5 (alpha-beta)   | ~1h       |
| Tests et validation                | ~1h       |
| Documentation                      | ~1h       |
| **Total Semaine 3**                | **~5.5h** |

**Comparaison avec planning:** Pr√©vu 12-15h, r√©alis√© en ~5.5h gr√¢ce √† la bonne pr√©paration des semaines pr√©c√©dentes et l'aide de l'IA pour le debugging.
