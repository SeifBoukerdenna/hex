### Semaine 4: Iterative Deepening et Optimisations

#### ‚úÖ T√¢ches compl√©t√©es

- [x] Alpha-beta pruning (fait en semaine 3)
- [x] Impl√©menter iterative deepening
- [x] Gestion du temps par coup
- [x] Tester v0.6 vs v0.5
- [x] Valider performances vs random et greedy

---

## Comparaison v0.5 vs v0.6

### Architecture v0.5 (Alpha-Beta fixe)

```python
self.max_depth = 2  # Toujours profondeur 2
# Explore jusqu'√† profondeur 2, peu importe le temps
```

**Probl√®mes:**

- Profondeur fixe = temps variable (19s √† 50s)
- Pas de contr√¥le du budget temps
- Risque de timeout en comp√©tition

### Architecture v0.6 (Iterative Deepening)

```python
self.max_depth = 10  # Limite haute
self.time_per_move = 5.0  # Budget par coup

for depth in range(1, self.max_depth + 1):
    try:
        # Chercher √† cette profondeur
        # Si timeout ‚Üí break et retourner meilleur coup trouv√©
    except TimeoutError:
        break
```

**Avantages:**

- Temps contr√¥l√© (~5s par coup)
- Toujours un coup valide (profondeur 1 garantie)
- Utilise le temps disponible intelligemment
- Plus profond quand la position est simple

---

## Impl√©mentation d√©taill√©e

### Iterative Deepening

```python
def compute_action(self, current_state, **kwargs):
    start_time = time.time()
    best_action = possible_actions[0]  # Fallback

    for depth in range(1, self.max_depth + 1):
        try:
            # Chercher √† profondeur `depth`
            current_best = self.search_at_depth(depth, start_time)
            best_action = current_best  # Sauvegarder si r√©ussi
        except TimeoutError:
            break  # Temps √©coul√©, retourner dernier best_action

    return best_action
```

### V√©rification du temps dans alpha-beta

```python
def alphabeta(self, state, depth, alpha, beta, maximizing, start_time):
    if time.time() - start_time > self.time_per_move:
        raise TimeoutError()  # Interruption propre
    # ... reste de l'algorithme
```

### Param√®tres cl√©s

| Param√®tre       | Valeur | Justification                        |
| --------------- | ------ | ------------------------------------ |
| `max_depth`     | 10     | Limite th√©orique (rarement atteinte) |
| `time_per_move` | 5.0s   | ~60 coups √ó 5s = 300s << 900s budget |

---

## üìä R√©sultats des tests v0.6

### TEST 1: v0.6 vs Random (10 parties)

**Configuration Rouge (premier):**
| Partie | R√©sultat | Temps |
|--------|----------|-------|
| 1 | ‚úì Victoire | 20.6s |
| 2 | ‚úì Victoire | 23.1s |
| 3 | ‚úì Victoire | 27.3s |
| 4 | ‚úì Victoire | 26.7s |
| 5 | ‚úì Victoire | 31.4s |
| **Total** | **5/5 (100%)** | **Moy: 25.8s** |

**Configuration Bleu (second):**
| Partie | R√©sultat | Temps |
|--------|----------|-------|
| 1 | ‚úì Victoire | 30.1s |
| 2 | ‚úì Victoire | 39.9s |
| 3 | ‚úì Victoire | 20.4s |
| 4 | ‚úì Victoire | 23.4s |
| 5 | ‚úì Victoire | 23.0s |
| **Total** | **5/5 (100%)** | **Moy: 27.4s** |

**R√©sultat global vs Random: 10/10 (100%)**

### TEST 2: v0.6 vs Greedy (10 parties)

**Configuration Rouge (premier):**
| Partie | R√©sultat | Temps |
|--------|----------|-------|
| 1 | ‚úì Victoire | 24.0s |
| 2 | ‚úì Victoire | 29.9s |
| 3 | ‚úì Victoire | 23.7s |
| 4 | ‚úì Victoire | 26.8s |
| 5 | ‚úì Victoire | 25.0s |
| **Total** | **5/5 (100%)** | **Moy: 25.9s** |

**Configuration Bleu (second):**
| Partie | R√©sultat | Temps |
|--------|----------|-------|
| 1 | ‚úì Victoire | 34.2s |
| 2 | ‚úì Victoire | 35.6s |
| 3 | ‚úì Victoire | 36.7s |
| 4 | ‚úì Victoire | 37.2s |
| 5 | ‚úì Victoire | 36.8s |
| **Total** | **5/5 (100%)** | **Moy: 36.1s** |

**R√©sultat global vs Greedy: 10/10 (100%)**

### TEST 3: v0.6 vs v0.5 (10 parties)

| Configuration           | Gagnant        | Temps moy |
| ----------------------- | -------------- | --------- |
| v0.6 Rouge vs v0.5 Bleu | v0.6 (5/5)     | 153.9s    |
| v0.5 Rouge vs v0.6 Bleu | v0.5 (5/5)     | 154.9s    |
| **Total**               | **5/10 (50%)** | -         |

**Analyse:** 50% = force √©gale. Le premier joueur gagne toujours car les deux agents ont la m√™me strat√©gie. Cela confirme que v0.6 n'est pas plus faible que v0.5.

---

## üìà Tableau comparatif complet

| Version  | vs Random | vs Greedy | vs v0.5 | Temps/partie | Contr√¥le temps |
| -------- | --------- | --------- | ------- | ------------ | -------------- |
| v0.5     | 100%      | 100%      | -       | ~30s         | ‚ùå Non         |
| **v0.6** | **100%**  | **100%**  | **50%** | **~28s**     | **‚úÖ Oui**     |

### Am√©lioration cl√©: Gestion du temps

**v0.5:**

- Temps impr√©visible (19s √† 50s)
- Risque de d√©passer le budget de 15 min
- Pas d'adaptation √† la complexit√©

**v0.6:**

- Temps contr√¥l√© (~5s par coup max)
- Budget garanti: 60 coups √ó 5s = 300s << 900s
- Peut chercher plus profond sur positions simples

---

## ü§î Analyse approfondie

### Pourquoi v0.6 = v0.5 en force?

Les deux versions utilisent:

1. M√™me heuristique (distance_adversaire - ma_distance)
2. M√™me alpha-beta pruning
3. M√™me tri des actions (centre d'abord)

La seule diff√©rence est la gestion du temps. Avec 5s/coup, v0.6 atteint g√©n√©ralement profondeur 2 comme v0.5, d'o√π les performances identiques.

### Pourquoi le temps varie (20s √† 40s par partie)?

Facteurs:

1. **Nombre de coups:** Parties courtes = moins de temps total
2. **Complexit√© des positions:** Certaines n√©cessitent plus d'exploration
3. **Efficacit√© du pruning:** Variable selon l'ordre des coups

### Calcul du budget temps

```
Budget total: 15 min = 900 secondes
Coups typiques: 40-80 par partie
Temps par coup: 5s

Pire cas: 80 coups √ó 5s = 400s (44% du budget)
Cas moyen: 60 coups √ó 5s = 300s (33% du budget)

Marge de s√©curit√©: >50% du budget inutilis√© ‚úì
```

---

## üí° Optimisations futures possibles

### 1. Augmenter time_per_move

```python
self.time_per_move = 10.0  # Doubler le temps
```

- Permettrait profondeur 3 plus souvent
- Toujours dans le budget (600s < 900s)

### 2. Temps adaptatif selon la phase

```python
def get_time_for_move(self, step):
    if step < 20:
        return 8.0   # D√©but: positions cruciales
    elif step < 60:
        return 5.0   # Milieu: temps standard
    else:
        return 2.0   # Fin: positions simples
```

### 3. Am√©liorer l'heuristique

```python
def evaluate(self, state):
    score = opponent_distance - my_distance

    # Bonus centre
    for pos in my_pieces:
        score += 0.1 * (7 - abs(pos[0]-6.5) - abs(pos[1]-6.5))

    # Bonus connectivit√©
    score += 0.5 * connected_groups_bonus

    return score
```

### 4. Transposition table

```python
self.transposition_table = {}

def alphabeta(self, state, ...):
    state_hash = self.hash_state(state)
    if state_hash in self.transposition_table:
        return self.transposition_table[state_hash]
    # ... calcul
    self.transposition_table[state_hash] = result
```

---

## üéØ Prochaines √©tapes recommand√©es

### Priorit√© 1: Soumettre sur Abyss

- Tester contre de vrais adversaires
- Identifier faiblesses contre agents humains
- Obtenir classement Elo initial

### Priorit√© 2: Pr√©parer agent concours (16 nov)

- Nettoyer le code
- Supprimer prints de debug
- V√©rifier requirements.txt
- Tester sur environnement propre

### Priorit√© 3: Am√©liorations optionnelles

- Heuristique am√©lior√©e
- Temps adaptatif
- Plus de tests statistiques

---

## üìù Structure du code v0.6

```
my_player.py (v0.6)
‚îú‚îÄ‚îÄ __init__
‚îÇ   ‚îú‚îÄ‚îÄ piece_type, opponent_type
‚îÇ   ‚îú‚îÄ‚îÄ max_depth = 10
‚îÇ   ‚îî‚îÄ‚îÄ time_per_move = 5.0
‚îú‚îÄ‚îÄ compute_action
‚îÇ   ‚îú‚îÄ‚îÄ Tri des actions (centre first)
‚îÇ   ‚îî‚îÄ‚îÄ Iterative deepening avec timeout
‚îú‚îÄ‚îÄ alphabeta
‚îÇ   ‚îú‚îÄ‚îÄ V√©rification temps
‚îÇ   ‚îú‚îÄ‚îÄ Alpha-beta pruning
‚îÇ   ‚îî‚îÄ‚îÄ Tri des actions
‚îú‚îÄ‚îÄ evaluate
‚îÇ   ‚îî‚îÄ‚îÄ distance_adversaire - ma_distance
‚îî‚îÄ‚îÄ calculate_shortest_path
    ‚îî‚îÄ‚îÄ Dijkstra
```

---

## ‚è±Ô∏è Temps pass√© Semaine 4

| T√¢che                              | Temps   |
| ---------------------------------- | ------- |
| Impl√©mentation iterative deepening | ~1h     |
| Debug et corrections               | ~30min  |
| Tests v0.6                         | ~1h     |
| Documentation                      | ~30min  |
| **Total**                          | **~3h** |

---

## üí≠ R√©flexions finales

### Ce que v0.6 apporte

- ‚úÖ Gestion du temps robuste pour la comp√©tition
- ‚úÖ M√™me force que v0.5
- ‚úÖ Code pr√™t pour soumission Abyss

### Ce qui reste identique

- Heuristique (shortest path difference)
- Force de jeu (100% vs random/greedy)
- Alpha-beta avec tri centre

### D√©cision pour la comp√©tition

v0.6 est recommand√© pour la soumission car:

1. Gestion du temps garantit pas de timeout
2. Force identique √† v0.5
3. Architecture extensible pour futures am√©liorations

**Status:** Pr√™t pour soumission Abyss et concours du 16 novembre.
